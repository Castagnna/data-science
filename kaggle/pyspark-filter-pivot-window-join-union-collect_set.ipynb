{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming col names\n",
    "somedata = (\n",
    "    somedata\n",
    "    .withColumnRenamed('col 1','col_1')\n",
    "    .withColumnRenamed('col 2','col_2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "somedata[\n",
    "    (somedata.col_1 == 'H') &\n",
    "    (somedata.col_2 == 'United States of America')\n",
    "].show(5,False)\n",
    "\n",
    "# alternative 1\n",
    "(\n",
    "    somedata\n",
    "    .filter(somedata.col_1 == 'Online')\n",
    "    .filter(somedata.col_2 == 'North America')\n",
    "    .show(5,False)\n",
    ")\n",
    "\n",
    "# alternative 2\n",
    "(\n",
    "    somedata\n",
    "    .filter('col_5<5 and (col_1 <> col_3 or (col_1 = col_3 and col_2 <> col_4))')\n",
    "    .show()\n",
    ")\n",
    "\n",
    "# alternative 3\n",
    "(\n",
    "    somedata\n",
    "    .where(((somedata.col_1 > 5000) | (somedata.col_2 >= 100000))\n",
    "           & (somedata.col_3 >= 1000000)\n",
    "    )\n",
    "    .show(5,False)\n",
    ")\n",
    "\n",
    "# with sql.functions - use `|` instead of `or`\n",
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "somedata = (\n",
    "    somedata\n",
    "    .filter((col(\"col_1\") == \"xiangrui\") | (col(\"col_2\") == \"michael\"))\n",
    "    .sort(asc(\"col_1\"))\n",
    ")\n",
    "display(somedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby and agg\n",
    "(\n",
    "    somedata\n",
    "    .groupby(['col_1', 'col_2'])\n",
    "    .agg(F.sum('col_1').alias('total_value'))\n",
    "    #.agg(F.mean('col_1').alias('mean_value'))\n",
    "    .show(5,False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect set\n",
    "(\n",
    "    somedata\n",
    "    .groupby('col_1')\n",
    "    .agg(F.collect_set('col_with_list_values'))\n",
    "    .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "join_data = somedata.join(otherdata, on='col_with_same_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union\n",
    "union_data = somedata_1.union(somedata_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "(\n",
    "    somedata\n",
    "    .groupby('col_1')\n",
    "    .pivot('col_2')\n",
    "    .sum('col_value')\n",
    "    .fillna(0)\n",
    "    .show(5,False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window function\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,row_number\n",
    "\n",
    "# creating a window\n",
    "my_window = Window.orderBy(somedata['col_1'].desc())\n",
    "\n",
    "# ranking over the window\n",
    "somedata = somedata.withColumn('rank',row_number().over(my_window).alias('rank'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
